{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bd7d37b-8a6a-42a2-b1ff-75d423ce1af7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02a669e-1a08-43c4-93df-9a3915a6a26c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4e048a1-834e-412d-b54d-45628520af3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read authentication credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d401c3-b2a0-46b2-af83-f1fda4ea8187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimeter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    ".option(\"header\", first_row_is_header)\\\n",
    ".option(\"sep\", delimiter)\\\n",
    ".load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477cb593-d9a8-4abf-900c-79333982efd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "#print(ACCESS_KEY, ', ', SECRET_KEY)\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00a4fc7e-3c4f-4a9d-8d1b-190bd106b8d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read data streams from AWS Kinesis into dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547fe257-9c48-408a-9139-c1ddcbe85e62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*************************************************\n",
    "# 1. PINTEREST DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_stream_pin = spark \\\n",
    "                .readStream \\\n",
    "                .format('kinesis') \\\n",
    "                .option('streamName','streaming-0a3c6c045333-pin') \\\n",
    "                .option('initialPosition','earliest') \\\n",
    "                .option('region','us-east-1') \\\n",
    "                .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                .option('awsSecretKey', SECRET_KEY) \\\n",
    "                .load()\n",
    "\n",
    "#*************************************************\n",
    "# 2. GEOLOCATION DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_stream_geo = spark \\\n",
    "                .readStream \\\n",
    "                .format('kinesis') \\\n",
    "                .option('streamName','streaming-0a3c6c045333-geo') \\\n",
    "                .option('initialPosition','earliest') \\\n",
    "                .option('region','us-east-1') \\\n",
    "                .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                .option('awsSecretKey', SECRET_KEY) \\\n",
    "                .load()\n",
    "\n",
    "#*************************************************\n",
    "# 3. USER DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_stream_user = spark \\\n",
    "                .readStream \\\n",
    "                .format('kinesis') \\\n",
    "                .option('streamName','streaming-0a3c6c045333-user') \\\n",
    "                .option('initialPosition','earliest') \\\n",
    "                .option('region','us-east-1') \\\n",
    "                .option('awsAccessKey', ACCESS_KEY) \\\n",
    "                .option('awsSecretKey', SECRET_KEY) \\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d35bc1-b3ad-45f9-ae6a-c1bbfc1311ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data deserialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eafac8d-19fc-48b6-904c-2e8ecff73762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*************************************************\n",
    "# 1. PINTEREST DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_pin_schema = StructType([\n",
    "    StructField(\"index\", StringType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", StringType()),  # Assuming downloaded is of StringType\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "df_pin = df_stream_pin \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\\n",
    "    .select(from_json(col(\"data\"), df_pin_schema).alias(\"pin_data\")) \\\n",
    "    .select(\"pin_data.*\")\n",
    "\n",
    "#*************************************************\n",
    "# 2. GEOLOCATION DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_geo_schema = StructType([\n",
    "    StructField(\"ind\", StringType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "df_geo = df_stream_geo \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\\n",
    "    .select(from_json(col(\"data\"), df_geo_schema).alias(\"geo_data\")) \\\n",
    "    .select(\"geo_data.*\")\n",
    "\n",
    "#*************************************************\n",
    "# 3. USER DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_user_schema = StructType([\n",
    "    StructField(\"ind\", StringType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"date_joined\", TimestampType())\n",
    "])\n",
    "\n",
    "df_user = df_stream_user \\\n",
    "    .selectExpr(\"CAST(data as STRING)\") \\\n",
    "    .select(from_json(col(\"data\"), df_user_schema).alias(\"user_data\")) \\\n",
    "    .select(\"user_data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f93e2a71-b302-411b-bc29-66d296209059",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e233e5-74e1-4503-82cc-49346625ee25",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*************************************************\n",
    "# 1. PINTEREST DATA CLEANING:\n",
    "#*************************************************\n",
    "\n",
    "# Replace empty entries and entries with no relevant data in each column with Nones\n",
    "df_pin_cleaned = df_pin.withColumn(\"category\", when(df_pin[\"category\"].isNull(), None).otherwise(df_pin[\"category\"]))\n",
    "df_pin_cleaned = df_pin.withColumn(\"description\", when(df_pin[\"description\"].isNull(), None).otherwise(df_pin[\"description\"]))\n",
    "df_pin_cleaned = df_pin.withColumn(\"downloaded\", when(df_pin[\"downloaded\"].isNull(), None).otherwise(df_pin[\"downloaded\"]))\n",
    "df_pin_cleaned = df_pin.withColumn(\"follower_count\", when(df_pin[\"follower_count\"].isNull(), None).otherwise(df_pin[\"follower_count\"]))\n",
    "# Perform the necessary transformations on the follower_count to ensure every entry is a number. Make sure the data type of this column is an int.\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\", \n",
    "                                when(col(\"follower_count\").endswith(\"k\"), regexp_replace(col(\"follower_count\"), \"k\", \"\").cast(\"int\")*1000)\n",
    "                                .when(col(\"follower_count\").endswith(\"k\"), regexp_replace(col(\"follower_count\"), \"M\", \"\").cast(\"int\")*1000000)\n",
    "                                .otherwise(col(\"follower_count\").cast(\"int\"))\n",
    "                                )\n",
    "# Ensure that each column containing numeric data has a numeric data type\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"downloaded\",col(\"downloaded\").cast(\"int\"))\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"follower_count\",col(\"follower_count\").cast(\"int\"))\n",
    "# Clean the data in the save_location column to include only the save location path\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"save_location\", regexp_replace(col(\"save_location\"), \"^Local save in \", \"\"))\n",
    "# Rename the index column to ind\n",
    "df_pin_cleaned = df_pin_cleaned.withColumnRenamed(\"index\", \"ind\")\n",
    "df_pin_cleaned = df_pin_cleaned.withColumn(\"ind\", col(\"ind\").cast(\"int\"))\n",
    "# Reorder the DataFrame columns\n",
    "pin_columns_reorder = [ \"ind\",\n",
    "                        \"unique_id\",\n",
    "                        \"title\",\n",
    "                        \"description\",\n",
    "                        \"follower_count\",\n",
    "                        \"poster_name\",\n",
    "                        \"tag_list\",\n",
    "                        \"is_image_or_video\",\n",
    "                        \"image_src\",\n",
    "                        \"save_location\",\n",
    "                        \"category\"\n",
    "                    ]\n",
    "df_pin_cleaned = df_pin_cleaned.select(pin_columns_reorder)\n",
    "\n",
    "#*************************************************\n",
    "# 2. GEOLOCATION DATA CLEANINIG:\n",
    "#*************************************************\n",
    "\n",
    "# Create a new column coordinates that contains an array based on the latitude and longitude columns\n",
    "df_geo_cleaned = df_geo.withColumn(\"coordinates\", array(col(\"latitude\"), col(\"longitude\")))\n",
    "# Drop the latitude and longitude columns from the DataFrame\n",
    "df_geo_cleaned = df_geo_cleaned.drop(\"latitude\", \"longitude\")\n",
    "# Convert the timestamp column from a string to a timestamp data type\n",
    "df_geo_cleaned = df_geo_cleaned.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "# Reorder the DataFrame columns\n",
    "geo_columns_reorder = [ \"ind\",\n",
    "                        \"country\",\n",
    "                        \"coordinates\",\n",
    "                        \"timestamp\"\n",
    "                    ]\n",
    "df_geo_cleaned = df_geo_cleaned.select(geo_columns_reorder)\n",
    "\n",
    "#*************************************************\n",
    "# 3. USER DATA CLEANING:\n",
    "#*************************************************\n",
    "\n",
    "# Create a new column user_name that concatenates the information found in the first_name and last_name columns\n",
    "df_user_cleaned = df_user.withColumn(\"user_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\")))\n",
    "# Drop the first_name and last_name columns from the DataFrame\n",
    "df_user_cleaned = df_user_cleaned.drop(\"first_name\", \"last_name\")\n",
    "# Convert the date_joined column from a string to a timestamp data type\n",
    "df_user_cleaned = df_user_cleaned.withColumn(\"date_joined\", to_timestamp(\"date_joined\"))\n",
    "# Reorder the DataFrame columns\n",
    "user_columns_reorder = [ \"ind\",\n",
    "                        \"user_name\",\n",
    "                        \"age\",\n",
    "                        \"date_joined\"\n",
    "                    ]\n",
    "df_user_cleaned = df_user_cleaned.select(user_columns_reorder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b1ddd6-84d4-49c2-8f97-f8554c3154e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Display data streams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b8e33fa-3ddb-4f5d-8819-5f0ddd0190b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_pin_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599fabd3-aaeb-42cf-bb2f-c48e8eade8ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_geo_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3939a724-b29d-4135-8f50-511bd8bc96b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_user_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6786d819-126e-4138-8630-027ea3a8fef9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write data streams to Databricks (Delta Tables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7006dc-e5b0-442c-a94a-62c633a541ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[14]: True</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[14]: True</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/\", True)\n",
    "#dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/pin_table\", True)\n",
    "#dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/geo_table\", True)\n",
    "#dbutils.fs.rm(\"/tmp/kinesis/_checkpoints/user_table\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1f46ab-04fa-4ba3-8201-7d35597675ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[15]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fe4236a97f0&gt;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[15]: &lt;pyspark.sql.streaming.StreamingQuery at 0x7fe4236a97f0&gt;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#*************************************************\n",
    "# 1. WRITE PINTEREST DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_pin_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/pin_table\") \\\n",
    "  .table(\"0a3c6c045333_pin_table\")\n",
    "\n",
    "#*************************************************\n",
    "# 2. WRITE GEOLOCATION DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_geo_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/geo_table\") \\\n",
    "  .table(\"0a3c6c045333_geo_table\")\n",
    "\n",
    "#*************************************************\n",
    "# 3. WRITE USER DATA:\n",
    "#*************************************************\n",
    "\n",
    "df_user_cleaned.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/user_table\") \\\n",
    "  .table(\"0a3c6c045333_user_table\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream-data_kinesis_read-transform-write",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
